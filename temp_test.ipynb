{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bc7f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "class TimmVision(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    TimmVision module to allow loading any timm model with optional features_only output.\n",
    "\n",
    "    Attributes:\n",
    "        m (nn.Module): The loaded timm model or feature extractor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        pretrained: bool = True,\n",
    "        unwrap: bool = True,\n",
    "        truncate: int = 2,\n",
    "        split: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (str): Name of the timm model to load.\n",
    "            pretrained (bool): Whether to load pretrained weights.\n",
    "            unwrap (bool): Whether to unwrap into Sequential layers.\n",
    "            truncate (int): Number of layers to remove from the end if unwrap=True.\n",
    "            split (bool): If True, returns intermediate feature maps using timm's features_only.\n",
    "        \"\"\"\n",
    "        import timm \n",
    "        super().__init__()\n",
    "\n",
    "        self.split = split\n",
    "\n",
    "        if split:\n",
    "            # Use timm's features_only to get intermediate features\n",
    "            self.m = timm.create_model(model, pretrained=pretrained, features_only=True)\n",
    "        else:\n",
    "            # Standard model\n",
    "            self.m = timm.create_model(model, pretrained=pretrained)\n",
    "            if unwrap:\n",
    "                # Break model into children layers\n",
    "                layers = list(self.m.children())\n",
    "                if isinstance(layers[0], nn.Sequential):  # nested Sequential\n",
    "                    layers = [*list(layers[0].children()), *layers[1:]]\n",
    "                # Truncate last `truncate` layers\n",
    "                self.m = nn.Sequential(*(layers[:-truncate] if truncate else layers))\n",
    "            else:\n",
    "                # Remove classifier / head\n",
    "                for attr in [\"classifier\", \"fc\", \"head\"]:\n",
    "                    if hasattr(self.m, attr):\n",
    "                        setattr(self.m, attr, nn.Identity())\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.split:\n",
    "            # timm features_only returns list of feature maps\n",
    "            return self.m(x)\n",
    "        else:\n",
    "            return self.m(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf2acdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "['cspresnet50.ra_in1k',\n",
      " 'eca_resnet33ts.ra2_in1k',\n",
      " 'ecaresnet26t.ra2_in1k',\n",
      " 'ecaresnet50d.miil_in1k',\n",
      " 'ecaresnet50d_pruned.miil_in1k',\n",
      " 'ecaresnet50t.a1_in1k',\n",
      " 'ecaresnet50t.a2_in1k',\n",
      " 'ecaresnet50t.a3_in1k',\n",
      " 'ecaresnet50t.ra2_in1k',\n",
      " 'ecaresnet101d.miil_in1k',\n",
      " 'ecaresnet101d_pruned.miil_in1k',\n",
      " 'ecaresnet269d.ra2_in1k',\n",
      " 'ecaresnetlight.miil_in1k',\n",
      " 'gcresnet33ts.ra2_in1k',\n",
      " 'gcresnet50t.ra2_in1k',\n",
      " 'inception_resnet_v2.tf_ens_adv_in1k',\n",
      " 'inception_resnet_v2.tf_in1k',\n",
      " 'lambda_resnet26rpt_256.c1_in1k',\n",
      " 'lambda_resnet26t.c1_in1k',\n",
      " 'lambda_resnet50ts.a1h_in1k',\n",
      " 'legacy_seresnet18.in1k',\n",
      " 'legacy_seresnet34.in1k',\n",
      " 'legacy_seresnet50.in1k',\n",
      " 'legacy_seresnet101.in1k',\n",
      " 'legacy_seresnet152.in1k',\n",
      " 'nf_resnet50.ra2_in1k',\n",
      " 'resnet10t.c3_in1k',\n",
      " 'resnet14t.c3_in1k',\n",
      " 'resnet18.a1_in1k',\n",
      " 'resnet18.a2_in1k',\n",
      " 'resnet18.a3_in1k',\n",
      " 'resnet18.fb_ssl_yfcc100m_ft_in1k',\n",
      " 'resnet18.fb_swsl_ig1b_ft_in1k',\n",
      " 'resnet18.gluon_in1k',\n",
      " 'resnet18.tv_in1k',\n",
      " 'resnet18d.ra2_in1k',\n",
      " 'resnet18d.ra4_e3600_r224_in1k',\n",
      " 'resnet26.bt_in1k',\n",
      " 'resnet26d.bt_in1k',\n",
      " 'resnet26t.ra2_in1k',\n",
      " 'resnet32ts.ra2_in1k',\n",
      " 'resnet33ts.ra2_in1k',\n",
      " 'resnet34.a1_in1k',\n",
      " 'resnet34.a2_in1k',\n",
      " 'resnet34.a3_in1k',\n",
      " 'resnet34.bt_in1k',\n",
      " 'resnet34.gluon_in1k',\n",
      " 'resnet34.ra4_e3600_r224_in1k',\n",
      " 'resnet34.tv_in1k',\n",
      " 'resnet34d.ra2_in1k',\n",
      " 'resnet50.a1_in1k',\n",
      " 'resnet50.a1h_in1k',\n",
      " 'resnet50.a2_in1k',\n",
      " 'resnet50.a3_in1k',\n",
      " 'resnet50.am_in1k',\n",
      " 'resnet50.b1k_in1k',\n",
      " 'resnet50.b2k_in1k',\n",
      " 'resnet50.bt_in1k',\n",
      " 'resnet50.c1_in1k',\n",
      " 'resnet50.c2_in1k',\n",
      " 'resnet50.d_in1k',\n",
      " 'resnet50.fb_ssl_yfcc100m_ft_in1k',\n",
      " 'resnet50.fb_swsl_ig1b_ft_in1k',\n",
      " 'resnet50.gluon_in1k',\n",
      " 'resnet50.ra_in1k',\n",
      " 'resnet50.ram_in1k',\n",
      " 'resnet50.tv2_in1k',\n",
      " 'resnet50.tv_in1k',\n",
      " 'resnet50_clip.cc12m',\n",
      " 'resnet50_clip.openai',\n",
      " 'resnet50_clip.yfcc15m',\n",
      " 'resnet50_clip_gap.cc12m',\n",
      " 'resnet50_clip_gap.openai',\n",
      " 'resnet50_clip_gap.yfcc15m',\n",
      " 'resnet50_gn.a1h_in1k',\n",
      " 'resnet50c.gluon_in1k',\n",
      " 'resnet50d.a1_in1k',\n",
      " 'resnet50d.a2_in1k',\n",
      " 'resnet50d.a3_in1k',\n",
      " 'resnet50d.gluon_in1k',\n",
      " 'resnet50d.ra2_in1k',\n",
      " 'resnet50d.ra4_e3600_r224_in1k',\n",
      " 'resnet50s.gluon_in1k',\n",
      " 'resnet50x4_clip.openai',\n",
      " 'resnet50x4_clip_gap.openai',\n",
      " 'resnet50x16_clip.openai',\n",
      " 'resnet50x16_clip_gap.openai',\n",
      " 'resnet50x64_clip.openai',\n",
      " 'resnet50x64_clip_gap.openai',\n",
      " 'resnet51q.ra2_in1k',\n",
      " 'resnet61q.ra2_in1k',\n",
      " 'resnet101.a1_in1k',\n",
      " 'resnet101.a1h_in1k',\n",
      " 'resnet101.a2_in1k',\n",
      " 'resnet101.a3_in1k',\n",
      " 'resnet101.gluon_in1k',\n",
      " 'resnet101.tv2_in1k',\n",
      " 'resnet101.tv_in1k',\n",
      " 'resnet101_clip.openai',\n",
      " 'resnet101_clip.yfcc15m',\n",
      " 'resnet101_clip_gap.openai',\n",
      " 'resnet101_clip_gap.yfcc15m',\n",
      " 'resnet101c.gluon_in1k',\n",
      " 'resnet101d.gluon_in1k',\n",
      " 'resnet101d.ra2_in1k',\n",
      " 'resnet101s.gluon_in1k',\n",
      " 'resnet152.a1_in1k',\n",
      " 'resnet152.a1h_in1k',\n",
      " 'resnet152.a2_in1k',\n",
      " 'resnet152.a3_in1k',\n",
      " 'resnet152.gluon_in1k',\n",
      " 'resnet152.tv2_in1k',\n",
      " 'resnet152.tv_in1k',\n",
      " 'resnet152c.gluon_in1k',\n",
      " 'resnet152d.gluon_in1k',\n",
      " 'resnet152d.ra2_in1k',\n",
      " 'resnet152s.gluon_in1k',\n",
      " 'resnet200d.ra2_in1k',\n",
      " 'resnetaa50.a1h_in1k',\n",
      " 'resnetaa50d.d_in12k',\n",
      " 'resnetaa50d.sw_in12k',\n",
      " 'resnetaa50d.sw_in12k_ft_in1k',\n",
      " 'resnetaa101d.sw_in12k',\n",
      " 'resnetaa101d.sw_in12k_ft_in1k',\n",
      " 'resnetblur50.bt_in1k',\n",
      " 'resnetrs50.tf_in1k',\n",
      " 'resnetrs101.tf_in1k',\n",
      " 'resnetrs152.tf_in1k',\n",
      " 'resnetrs200.tf_in1k',\n",
      " 'resnetrs270.tf_in1k',\n",
      " 'resnetrs350.tf_in1k',\n",
      " 'resnetrs420.tf_in1k',\n",
      " 'resnetv2_18.ra4_e3600_r224_in1k',\n",
      " 'resnetv2_18d.ra4_e3600_r224_in1k',\n",
      " 'resnetv2_34.ra4_e3600_r224_in1k',\n",
      " 'resnetv2_34d.ra4_e3600_r224_in1k',\n",
      " 'resnetv2_34d.ra4_e3600_r384_in1k',\n",
      " 'resnetv2_50.a1h_in1k',\n",
      " 'resnetv2_50d_evos.ah_in1k',\n",
      " 'resnetv2_50d_gn.ah_in1k',\n",
      " 'resnetv2_50x1_bit.goog_distilled_in1k',\n",
      " 'resnetv2_50x1_bit.goog_in21k',\n",
      " 'resnetv2_50x1_bit.goog_in21k_ft_in1k',\n",
      " 'resnetv2_50x3_bit.goog_in21k',\n",
      " 'resnetv2_50x3_bit.goog_in21k_ft_in1k',\n",
      " 'resnetv2_101.a1h_in1k',\n",
      " 'resnetv2_101x1_bit.goog_in21k',\n",
      " 'resnetv2_101x1_bit.goog_in21k_ft_in1k',\n",
      " 'resnetv2_101x3_bit.goog_in21k',\n",
      " 'resnetv2_101x3_bit.goog_in21k_ft_in1k',\n",
      " 'resnetv2_152x2_bit.goog_in21k',\n",
      " 'resnetv2_152x2_bit.goog_in21k_ft_in1k',\n",
      " 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k',\n",
      " 'resnetv2_152x2_bit.goog_teacher_in21k_ft_in1k_384',\n",
      " 'resnetv2_152x4_bit.goog_in21k',\n",
      " 'resnetv2_152x4_bit.goog_in21k_ft_in1k',\n",
      " 'seresnet33ts.ra2_in1k',\n",
      " 'seresnet50.a1_in1k',\n",
      " 'seresnet50.a2_in1k',\n",
      " 'seresnet50.a3_in1k',\n",
      " 'seresnet50.ra2_in1k',\n",
      " 'seresnet152d.ra2_in1k',\n",
      " 'skresnet18.ra_in1k',\n",
      " 'skresnet34.ra_in1k',\n",
      " 'test_resnet.r160_in1k',\n",
      " 'tresnet_l.miil_in1k',\n",
      " 'tresnet_l.miil_in1k_448',\n",
      " 'tresnet_m.miil_in1k',\n",
      " 'tresnet_m.miil_in1k_448',\n",
      " 'tresnet_m.miil_in21k',\n",
      " 'tresnet_m.miil_in21k_ft_in1k',\n",
      " 'tresnet_v2_l.miil_in21k',\n",
      " 'tresnet_v2_l.miil_in21k_ft_in1k',\n",
      " 'tresnet_xl.miil_in1k',\n",
      " 'tresnet_xl.miil_in1k_448',\n",
      " 'wide_resnet50_2.racm_in1k',\n",
      " 'wide_resnet50_2.tv2_in1k',\n",
      " 'wide_resnet50_2.tv_in1k',\n",
      " 'wide_resnet101_2.tv2_in1k',\n",
      " 'wide_resnet101_2.tv_in1k']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "model_names = timm.list_models('*resnet*', pretrained=True)\n",
    "print(len(model_names))\n",
    "pprint(model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f6f808",
   "metadata": {},
   "source": [
    "resnet18.a1_in1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5035c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0c477fd35040c48a7625a98ededfb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_resnet18 = TimmVision('resnet18.a1_in1k',pretrained=True,unwrap=True,truncate=2,split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac0fceb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 64, 122, 122])\n",
      "1 torch.Size([1, 64, 61, 61])\n",
      "2 torch.Size([1, 128, 31, 31])\n",
      "3 torch.Size([1, 256, 16, 16])\n",
      "4 torch.Size([1, 512, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,3,244,244)\n",
    "for i, j in enumerate(model_resnet18(x)):\n",
    "    print(i, j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cc993d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchVision(nn.Module):\n",
    "    \"\"\"\n",
    "    TorchVision module to allow loading any torchvision model.\n",
    "\n",
    "    This class provides a way to load a model from the torchvision library, optionally load pre-trained weights, and customize the model by truncating or unwrapping layers.\n",
    "\n",
    "    Attributes:\n",
    "        m (nn.Module): The loaded torchvision model, possibly truncated and unwrapped.\n",
    "\n",
    "    Args:\n",
    "        model (str): Name of the torchvision model to load.\n",
    "        weights (str, optional): Pre-trained weights to load. Default is \"DEFAULT\".\n",
    "        unwrap (bool, optional): If True, unwraps the model to a sequential containing all but the last `truncate` layers. Default is True.\n",
    "        truncate (int, optional): Number of layers to truncate from the end if `unwrap` is True. Default is 2.\n",
    "        split (bool, optional): Returns output from intermediate child modules as list. Default is False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model: str, weights: str = \"DEFAULT\", unwrap: bool = True, truncate: int = 2, split: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Load the model and weights from torchvision.\n",
    "\n",
    "        Args:\n",
    "            model (str): Name of the torchvision model to load.\n",
    "            weights (str): Pre-trained weights to load.\n",
    "            unwrap (bool): Whether to unwrap the model.\n",
    "            truncate (int): Number of layers to truncate.\n",
    "            split (bool): Whether to split the output.\n",
    "        \"\"\"\n",
    "        import torchvision  # scope for faster 'import ultralytics'\n",
    "\n",
    "        super().__init__()\n",
    "        if hasattr(torchvision.models, \"get_model\"):\n",
    "            self.m = torchvision.models.get_model(model, weights=weights)\n",
    "        else:\n",
    "            self.m = torchvision.models.__dict__[model](pretrained=bool(weights))\n",
    "        if unwrap:\n",
    "            layers = list(self.m.children())\n",
    "            if isinstance(layers[0], nn.Sequential):  # Second-level for some models like EfficientNet, Swin\n",
    "                layers = [*list(layers[0].children()), *layers[1:]]\n",
    "            self.m = nn.Sequential(*(layers[:-truncate] if truncate else layers))\n",
    "            self.split = split\n",
    "        else:\n",
    "            self.split = False\n",
    "            self.m.head = self.m.heads = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor | list[torch.Tensor]): Output tensor or list of tensors.\n",
    "        \"\"\"\n",
    "        if self.split:\n",
    "            y = [x]\n",
    "            y.extend(m(y[-1]) for m in self.m)\n",
    "        else:\n",
    "            y = self.m(x)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "699fe756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 3, 244, 244])\n",
      "1 torch.Size([1, 96, 61, 61])\n",
      "2 torch.Size([1, 96, 61, 61])\n",
      "3 torch.Size([1, 192, 30, 30])\n",
      "4 torch.Size([1, 192, 30, 30])\n",
      "5 torch.Size([1, 384, 15, 15])\n",
      "6 torch.Size([1, 384, 15, 15])\n",
      "7 torch.Size([1, 768, 7, 7])\n",
      "8 torch.Size([1, 768, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.randn(1,3,244,244)\n",
    "model_convnext_tiny = TorchVision( 'convnext_tiny', 'DEFAULT', True, 2, True)\n",
    "\n",
    "for i, j in enumerate(model_convnext_tiny(x1)):\n",
    "    print(i, j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d96259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mobilenet_edgetpu_v2_m.ra4_e3600_r224_in1k',\n",
      " 'mobilenetv1_100.ra4_e3600_r224_in1k',\n",
      " 'mobilenetv1_100h.ra4_e3600_r224_in1k',\n",
      " 'mobilenetv1_125.ra4_e3600_r224_in1k',\n",
      " 'mobilenetv2_050.lamb_in1k',\n",
      " 'mobilenetv2_100.ra_in1k',\n",
      " 'mobilenetv2_110d.ra_in1k',\n",
      " 'mobilenetv2_120d.ra_in1k',\n",
      " 'mobilenetv2_140.ra_in1k',\n",
      " 'mobilenetv3_large_100.miil_in21k',\n",
      " 'mobilenetv3_large_100.miil_in21k_ft_in1k',\n",
      " 'mobilenetv3_large_100.ra4_e3600_r224_in1k',\n",
      " 'mobilenetv3_large_100.ra_in1k',\n",
      " 'mobilenetv3_large_150d.ra4_e3600_r256_in1k',\n",
      " 'mobilenetv3_rw.rmsp_in1k',\n",
      " 'mobilenetv3_small_050.lamb_in1k',\n",
      " 'mobilenetv3_small_075.lamb_in1k',\n",
      " 'mobilenetv3_small_100.lamb_in1k',\n",
      " 'mobilenetv4_conv_aa_large.e230_r384_in12k',\n",
      " 'mobilenetv4_conv_aa_large.e230_r384_in12k_ft_in1k',\n",
      " 'mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k',\n",
      " 'mobilenetv4_conv_aa_large.e600_r384_in1k',\n",
      " 'mobilenetv4_conv_blur_medium.e500_r224_in1k',\n",
      " 'mobilenetv4_conv_large.e500_r256_in1k',\n",
      " 'mobilenetv4_conv_large.e600_r384_in1k',\n",
      " 'mobilenetv4_conv_medium.e180_ad_r384_in12k',\n",
      " 'mobilenetv4_conv_medium.e180_r384_in12k',\n",
      " 'mobilenetv4_conv_medium.e250_r384_in12k',\n",
      " 'mobilenetv4_conv_medium.e250_r384_in12k_ft_in1k',\n",
      " 'mobilenetv4_conv_medium.e500_r224_in1k',\n",
      " 'mobilenetv4_conv_medium.e500_r256_in1k',\n",
      " 'mobilenetv4_conv_small.e1200_r224_in1k',\n",
      " 'mobilenetv4_conv_small.e2400_r224_in1k',\n",
      " 'mobilenetv4_conv_small.e3600_r256_in1k',\n",
      " 'mobilenetv4_conv_small_050.e3000_r224_in1k',\n",
      " 'mobilenetv4_hybrid_large.e600_r384_in1k',\n",
      " 'mobilenetv4_hybrid_large.ix_e600_r384_in1k',\n",
      " 'mobilenetv4_hybrid_medium.e200_r256_in12k',\n",
      " 'mobilenetv4_hybrid_medium.e200_r256_in12k_ft_in1k',\n",
      " 'mobilenetv4_hybrid_medium.e500_r224_in1k',\n",
      " 'mobilenetv4_hybrid_medium.ix_e550_r256_in1k',\n",
      " 'mobilenetv4_hybrid_medium.ix_e550_r384_in1k',\n",
      " 'mobilenetv5_300m.gemma3n',\n",
      " 'mobileone_s0.apple_in1k',\n",
      " 'mobileone_s1.apple_in1k',\n",
      " 'mobileone_s2.apple_in1k',\n",
      " 'mobileone_s3.apple_in1k',\n",
      " 'mobileone_s4.apple_in1k',\n",
      " 'mobilevit_s.cvnets_in1k',\n",
      " 'mobilevit_xs.cvnets_in1k',\n",
      " 'mobilevit_xxs.cvnets_in1k',\n",
      " 'mobilevitv2_050.cvnets_in1k',\n",
      " 'mobilevitv2_075.cvnets_in1k',\n",
      " 'mobilevitv2_100.cvnets_in1k',\n",
      " 'mobilevitv2_125.cvnets_in1k',\n",
      " 'mobilevitv2_150.cvnets_in1k',\n",
      " 'mobilevitv2_150.cvnets_in22k_ft_in1k',\n",
      " 'mobilevitv2_150.cvnets_in22k_ft_in1k_384',\n",
      " 'mobilevitv2_175.cvnets_in1k',\n",
      " 'mobilevitv2_175.cvnets_in22k_ft_in1k',\n",
      " 'mobilevitv2_175.cvnets_in22k_ft_in1k_384',\n",
      " 'mobilevitv2_200.cvnets_in1k',\n",
      " 'mobilevitv2_200.cvnets_in22k_ft_in1k',\n",
      " 'mobilevitv2_200.cvnets_in22k_ft_in1k_384',\n",
      " 'tf_mobilenetv3_large_075.in1k',\n",
      " 'tf_mobilenetv3_large_100.in1k',\n",
      " 'tf_mobilenetv3_large_minimal_100.in1k',\n",
      " 'tf_mobilenetv3_small_075.in1k',\n",
      " 'tf_mobilenetv3_small_100.in1k',\n",
      " 'tf_mobilenetv3_small_minimal_100.in1k']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "model_mobilenet_names = timm.list_models(\"*mobile*\", pretrained=True)\n",
    "\n",
    "pprint(model_mobilenet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c165e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d339cc6ccf14df086cf5e64b5db88af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/15.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (norm_head.num_batches_tracked, classifier.bias, classifier.weight, conv_head.weight, norm_head.bias, norm_head.running_mean, norm_head.running_var, norm_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 32, 122, 122])\n",
      "1 torch.Size([1, 32, 61, 61])\n",
      "2 torch.Size([1, 64, 31, 31])\n",
      "3 torch.Size([1, 96, 16, 16])\n",
      "4 torch.Size([1, 960, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "model_mobilenetv4 = TimmVision(\"mobilenetv4_conv_small.e1200_r224_in1k\",split=True)\n",
    "\n",
    "x = torch.randn(1,3,244,244)\n",
    "for i, j in enumerate(model_mobilenetv4(x)):\n",
    "    print(i, j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdc4af3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
